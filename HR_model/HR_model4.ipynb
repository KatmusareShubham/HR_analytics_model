{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing HR train file\n",
    "df = pd.read_csv('HR_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing on test data\n",
    "df2 = pd.read_csv('HR_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54808, 14)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employee_id                0\n",
       "department                 0\n",
       "region                     0\n",
       "education               2409\n",
       "gender                     0\n",
       "recruitment_channel        0\n",
       "no_of_trainings            0\n",
       "age                        0\n",
       "previous_year_rating    4124\n",
       "length_of_service          0\n",
       "KPIs_met >80%              0\n",
       "awards_won?                0\n",
       "avg_training_score         0\n",
       "is_promoted                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employee_id                0\n",
       "department                 0\n",
       "region                     0\n",
       "education               1034\n",
       "gender                     0\n",
       "recruitment_channel        0\n",
       "no_of_trainings            0\n",
       "age                        0\n",
       "previous_year_rating    1812\n",
       "length_of_service          0\n",
       "KPIs_met >80%              0\n",
       "awards_won?                0\n",
       "avg_training_score         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['education', 'previous_year_rating'], dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df = df.isnull().sum()\n",
    "missing_df[missing_df.values != 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_cat(df, variable):\n",
    "    # random sampling\n",
    "    #df[variable] = df[variable]\n",
    "        \n",
    "    # extract the random sample to fill the na\n",
    "    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), \n",
    "                                                 random_state=0)\n",
    "    \n",
    "    # pandas needs to have the same index in order to merge datasets\n",
    "    random_sample.index = df[df[variable].isnull()].index\n",
    "    \n",
    "    df.loc[df[variable].isnull(), variable] = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_num(df, variable):\n",
    "    median = df[variable].median()\n",
    "    df[variable] = df[variable].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null value imputation for the train dataset\n",
    "for col in missing_df[missing_df.values != 0].index:\n",
    "    if(df[col].dtype == 'O'):\n",
    "        impute_cat(df,col)\n",
    "    else:\n",
    "        impute_num(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null value imputation for the Test dataset\n",
    "for col in missing_df[missing_df.values != 0].index:\n",
    "    if(df[col].dtype == 'O'):\n",
    "        impute_cat(df2,col)\n",
    "    else:\n",
    "        impute_num(df2, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categerical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding\n",
    "\n",
    "prob_df = df.groupby(['department'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['dept_WOE'] = df.department.map(woe_labels)\n",
    "df2['dept_WOE'] = df2.department.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['region'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['region_WOE'] = df.region.map(woe_labels)\n",
    "df2['region_WOE'] = df2.region.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['education'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['education_WOE'] = df.education.map(woe_labels)\n",
    "df2['education_WOE'] = df2.education.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['gender'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['gender_WOE'] = df.gender.map(woe_labels)\n",
    "df2['gender_WOE'] = df2.gender.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['recruitment_channel'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['recruitment_channel_WOE'] = df.recruitment_channel.map(woe_labels)\n",
    "df2['recruitment_channel_WOE'] = df2.recruitment_channel.map(woe_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['no_of_trainings'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['trainings_WOE'] = df.no_of_trainings.map(woe_labels)\n",
    "df2['trainings_WOE'] = df2.no_of_trainings.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = df.groupby(['avg_training_score'])['is_promoted'].mean()\n",
    "prob_df = pd.DataFrame(prob_df)\n",
    "prob_df['Not_promoted'] = 1-prob_df.is_promoted\n",
    "\n",
    "\n",
    "prob_df.loc[prob_df['is_promoted'] == 0, 'is_promoted'] = 0.00001\n",
    "prob_df.loc[prob_df['Not_promoted'] == 0, 'Not_promoted'] = 0.00001\n",
    "\n",
    "prob_df['WoE'] = np.log(prob_df['is_promoted']/prob_df['Not_promoted'])\n",
    "\n",
    "woe_labels = prob_df['WoE'].to_dict()\n",
    "\n",
    "df['avg_training_score_WOE'] = df.avg_training_score.map(woe_labels)\n",
    "df2['avg_training_score_WOE'] = df2.avg_training_score.map(woe_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>department</th>\n",
       "      <th>region</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>...</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "      <th>is_promoted</th>\n",
       "      <th>dept_WOE</th>\n",
       "      <th>region_WOE</th>\n",
       "      <th>education_WOE</th>\n",
       "      <th>gender_WOE</th>\n",
       "      <th>recruitment_channel_WOE</th>\n",
       "      <th>trainings_WOE</th>\n",
       "      <th>avg_training_score_WOE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65438</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_7</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.555904</td>\n",
       "      <td>-2.126523</td>\n",
       "      <td>-2.235082</td>\n",
       "      <td>-2.314444</td>\n",
       "      <td>-2.376107</td>\n",
       "      <td>-2.336974</td>\n",
       "      <td>-3.250762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id         department    region         education gender  \\\n",
       "0        65438  Sales & Marketing  region_7  Master's & above      f   \n",
       "\n",
       "  recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
       "0            sourcing                1   35                   5.0   \n",
       "\n",
       "   length_of_service           ...            awards_won?  avg_training_score  \\\n",
       "0                  8           ...                      0                  49   \n",
       "\n",
       "   is_promoted  dept_WOE  region_WOE  education_WOE  gender_WOE  \\\n",
       "0            0 -2.555904   -2.126523      -2.235082   -2.314444   \n",
       "\n",
       "   recruitment_channel_WOE  trainings_WOE  avg_training_score_WOE  \n",
       "0                -2.376107      -2.336974               -3.250762  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['department']\n",
    "del df['region']\n",
    "del df['education']\n",
    "del df['gender']\n",
    "del df['recruitment_channel']\n",
    "del df['no_of_trainings']\n",
    "del df['avg_training_score']\n",
    "del df2['department']\n",
    "del df2['region']\n",
    "del df2['education']\n",
    "del df2['gender']\n",
    "del df2['recruitment_channel']\n",
    "del df2['no_of_trainings']\n",
    "del df2['avg_training_score']\n",
    "del df['employee_id']\n",
    "del df2['employee_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, [0,1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]].values\n",
    "y = df.iloc[:, 5].values\n",
    "\n",
    "X2 = df2.iloc[:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, \n",
    "                                                    random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.difference(['is_promoted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def feature_select(X, y, cols, cutoff):\n",
    "    Classifier = RandomForestClassifier(n_estimators = 150, random_state = 0)\n",
    "    Classifier.fit(X, y)\n",
    "    feat_imps = pd.concat([pd.DataFrame(cols, columns=['Features']),\n",
    "                       pd.DataFrame(Classifier.feature_importances_, columns=['Importances'])],\n",
    "                     axis=1)\n",
    "    feat_imps = feat_imps.sort_values(['Importances'], ascending=False)\n",
    "    feat_imps['Cumulative Importances'] = feat_imps['Importances'].cumsum()\n",
    "    feat_imps = feat_imps[feat_imps['Cumulative Importances'] < cutoff]\n",
    "    return feat_imps['Features'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols = feature_select(X, y, cols, 0.990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainings_WOE',\n",
       " 'KPIs_met >80%',\n",
       " 'gender_WOE',\n",
       " 'avg_training_score_WOE',\n",
       " 'education_WOE',\n",
       " 'age',\n",
       " 'awards_won?',\n",
       " 'recruitment_channel_WOE',\n",
       " 'dept_WOE',\n",
       " 'region_WOE',\n",
       " 'previous_year_rating']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[imp_cols].values\n",
    "y = df.iloc[:, 5].values\n",
    "X2=df2[imp_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, \n",
    "                                                    random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X2=sc.transform(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote ho rha hai \n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "X_test_res, y_test_res = sm.fit_sample(X_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', \n",
    "                            max_depth= 3,\n",
    "                            random_state = 2)\n",
    "dt.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for DTrees: \n",
      " [[ 9822  5227]\n",
      " [ 2067 12982]]\n",
      "Accuracy for DTrees: \n",
      " 0.7576583161671872\n",
      "Precision for DTrees: \n",
      " 0.7129441484979955\n",
      "Recall for DTrees: \n",
      " 0.8626486809754801\n",
      "f1_score for DTrees: \n",
      " 0.7806843466233688\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for DTrees: \\n',confusion_matrix(y_test_res, dt.predict(X_test_res)))\n",
    "print('Accuracy for DTrees: \\n',accuracy_score(y_test_res, dt.predict(X_test_res)))\n",
    "acc.append(accuracy_score(y_test_res, dt.predict(X_test_res)))\n",
    "print('Precision for DTrees: \\n',precision_score(y_test_res, dt.predict(X_test_res)))\n",
    "precision.append(precision_score(y_test_res, dt.predict(X_test_res)))\n",
    "print('Recall for DTrees: \\n',recall_score(y_test_res, dt.predict(X_test_res)))\n",
    "recall.append(recall_score(y_test_res, dt.predict(X_test_res)))\n",
    "print('f1_score for DTrees: \\n',f1_score(y_test_res, dt.predict(X_test_res)))\n",
    "f1.append(f1_score(y_test_res, dt.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=11, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', \n",
    "                                    random_state = 0)\n",
    "rf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for RF: \n",
      " [[14552   497]\n",
      " [ 2630 12419]]\n",
      "Accuracy for RF: \n",
      " 0.896106053558376\n",
      "Precision for RF: \n",
      " 0.9615205946113348\n",
      "Recall for RF: \n",
      " 0.8252375573127783\n",
      "f1_score for RF: \n",
      " 0.8881816556409798\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for RF: \\n',confusion_matrix(y_test_res, rf.predict(X_test_res)))\n",
    "print('Accuracy for RF: \\n',accuracy_score(y_test_res, rf.predict(X_test_res)))\n",
    "acc.append(accuracy_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('Precision for RF: \\n',precision_score(y_test_res, rf.predict(X_test_res)))\n",
    "precision.append(precision_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('Recall for RF: \\n',recall_score(y_test_res, rf.predict(X_test_res)))\n",
    "recall.append(recall_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('f1_score for RF: \\n',f1_score(y_test_res, rf.predict(X_test_res)))\n",
    "f1.append(f1_score(y_test_res, rf.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=11, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=31, random_state=40)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier(base_estimator=rf, n_estimators=31, \n",
    "                         algorithm='SAMME.R', random_state=40)\n",
    "adb.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for RF: \n",
      " [[14552   497]\n",
      " [ 2630 12419]]\n",
      "Accuracy for RF: \n",
      " 0.896106053558376\n",
      "Precision for RF: \n",
      " 0.9615205946113348\n",
      "Recall for RF: \n",
      " 0.8252375573127783\n",
      "f1_score for RF: \n",
      " 0.8881816556409798\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for RF: \\n',confusion_matrix(y_test_res, rf.predict(X_test_res)))\n",
    "print('Accuracy for RF: \\n',accuracy_score(y_test_res, rf.predict(X_test_res)))\n",
    "acc.append(accuracy_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('Precision for RF: \\n',precision_score(y_test_res, rf.predict(X_test_res)))\n",
    "precision.append(precision_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('Recall for RF: \\n',recall_score(y_test_res, rf.predict(X_test_res)))\n",
    "recall.append(recall_score(y_test_res, rf.predict(X_test_res)))\n",
    "print('f1_score for RF: \\n',f1_score(y_test_res, rf.predict(X_test_res)))\n",
    "f1.append(f1_score(y_test_res, rf.predict(X_test_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=adb.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final).to_csv(\"predicted4.csv\",header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
